{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54QWMtUayk9k"
      },
      "source": [
        "Aprende a consumir APIs REST desde Python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2484f36b",
        "outputId": "24e2fcf6-ddf9-432d-aeba-1033a340a40f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attempting to fetch SIATA target URL: https://www.siata.gov.co/siata_nuevo with increased timeout.\n",
            "Successfully fetched SIATA homepage (or its redirected version).\n",
            "\n",
            "--- Analyzing SIATA Homepage Content ---\n",
            "\n",
            "--- Summary of Findings ---\n",
            "Potential data sources found in <script> tags:\n",
            "- Potential data in script (first 100 chars): \n",
            "  window.dataLayer = window.dataLayer || [];\n",
            "  function gtag(){dataLayer.push(arguments);}\n",
            "  gtag('...\n",
            "No <iframe> tags with src attributes found.\n",
            "\n",
            "Potential data loading elements (divs) found:\n",
            "- Div with id/class: /['mapa_detalle_panel'] (first 50 chars of content): \n",
            "\n",
            "\n",
            " Capas \n",
            "\n",
            "\n",
            " Informaci√≥n \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "...\n",
            "- Div with id/class: /['tab-content', 'navbar-left', 'mapa_detalle_tabs_content', 'small', 'active_panel', 'font_size'] (first 50 chars of content): \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "...\n",
            "- Div with id/class: mapa_div/['panel_left_map'] (first 50 chars of content):  ...\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "\n",
        "# The previously identified redirect URL is a more stable target.\n",
        "# Using the last observed redirect target directly.\n",
        "SIATA_TARGET_URL = \"https://www.siata.gov.co/siata_nuevo\"\n",
        "\n",
        "print(f\"Attempting to fetch SIATA target URL: {SIATA_TARGET_URL} with increased timeout.\")\n",
        "\n",
        "siata_homepage_soup = None\n",
        "try:\n",
        "    # Directly attempt to fetch the redirected URL, allowing further redirects, with a longer timeout\n",
        "    response = requests.get(SIATA_TARGET_URL, allow_redirects=True, timeout=15) # Increased timeout to 15 seconds\n",
        "    response.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)\n",
        "\n",
        "    print(\"Successfully fetched SIATA homepage (or its redirected version).\")\n",
        "    siata_homepage_soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    print(\"\\n--- Analyzing SIATA Homepage Content ---\")\n",
        "\n",
        "    # 1. Examine script tags for embedded JSON data or data URLs\n",
        "    script_data_sources = []\n",
        "    for script in siata_homepage_soup.find_all('script'):\n",
        "        if script.string and ('json' in script.string.lower() or 'data' in script.string.lower()): # Broad search\n",
        "            # This is a very basic heuristic. More advanced parsing would be needed to extract actual JSON.\n",
        "            script_data_sources.append(f\"Potential data in script (first 100 chars): {script.string[:100]}...\")\n",
        "\n",
        "    # 2. Look for iframe tags and extract their src attributes\n",
        "    iframe_sources = []\n",
        "    for iframe in siata_homepage_soup.find_all('iframe'):\n",
        "        if 'src' in iframe.attrs:\n",
        "            iframe_sources.append(iframe['src'])\n",
        "\n",
        "    # 3. Identify any other HTML elements that seem to contain or dynamically load meteorological data.\n",
        "    potential_data_divs = []\n",
        "    for div in siata_homepage_soup.find_all('div', class_=lambda x: x and ('map' in x or 'data' in x)):\n",
        "        if div.get('id') or div.get('class'):\n",
        "            potential_data_divs.append(f\"Div with id/class: {div.get('id', '')}/{div.get('class', '')} (first 50 chars of content): {div.text[:50]}...\")\n",
        "\n",
        "    # Print a summary of findings\n",
        "    print(\"\\n--- Summary of Findings ---\")\n",
        "    if script_data_sources:\n",
        "        print(\"Potential data sources found in <script> tags:\")\n",
        "        for src in script_data_sources:\n",
        "            print(f\"- {src}\")\n",
        "    else:\n",
        "        print(\"No obvious embedded JSON or data structures found in <script> tags.\")\n",
        "\n",
        "    if iframe_sources:\n",
        "        print(\"\\nPotential data sources found in <iframe> src attributes:\")\n",
        "        for src in iframe_sources:\n",
        "            print(f\"- {src}\")\n",
        "    else:\n",
        "        print(\"No <iframe> tags with src attributes found.\")\n",
        "\n",
        "    if potential_data_divs:\n",
        "        print(\"\\nPotential data loading elements (divs) found:\")\n",
        "        for div_info in potential_data_divs:\n",
        "            print(f\"- {div_info}\")\n",
        "    else:\n",
        "        print(\"No specific data loading divs identified.\")\n",
        "\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Error fetching SIATA homepage: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred during parsing: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a650dbc4",
        "outputId": "2242f9e1-abcc-4458-8dc3-804e0274a8d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attempting to fetch data from SIATA operational URL: https://www.siata.gov.co/operacional/\n",
            "Successfully fetched SIATA operational page from https://www.siata.gov.co/operacional/.\n",
            "Title of the page: Listing folder\n",
            "HTML content parsed successfully.\n",
            "\n",
            "No direct downloadable data links (.csv, .xlsx, .json, etc.) found on the page.\n",
            "Manual inspection of the page or network requests might be necessary to locate the data source.\n",
            "Failed to download SIATA data or no direct download links found.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "# 1. Define the URL for SIATA's operational data section\n",
        "SIATA_OPERACIONAL_URL = \"https://www.siata.gov.co/operacional/\"\n",
        "\n",
        "print(f\"Attempting to fetch data from SIATA operational URL: {SIATA_OPERACIONAL_URL}\")\n",
        "\n",
        "siata_operacional_soup = None\n",
        "siata_downloaded_data = None # Variable to store raw file content if downloaded\n",
        "\n",
        "try:\n",
        "    # 2. Make an HTTP GET request to the URL, allowing redirects and including a timeout\n",
        "    response = requests.get(SIATA_OPERACIONAL_URL, allow_redirects=True, timeout=20)\n",
        "    # 3. Implement robust error handling\n",
        "    response.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)\n",
        "\n",
        "    print(f\"Successfully fetched SIATA operational page from {SIATA_OPERACIONAL_URL}.\")\n",
        "\n",
        "    # 4. Parse the HTML content of the response\n",
        "    siata_operacional_soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    print(f\"Title of the page: {siata_operacional_soup.title.string if siata_operacional_soup.title else 'No title found'}\")\n",
        "    print(\"HTML content parsed successfully.\")\n",
        "\n",
        "    # 5. Inspect the parsed HTML to identify potential links to downloadable data files\n",
        "    # This is a placeholder for detailed inspection. We'll look for common download link patterns.\n",
        "    download_links = []\n",
        "    for link in siata_operacional_soup.find_all('a', href=True):\n",
        "        href = link['href']\n",
        "        # Look for common file extensions or keywords indicating data downloads\n",
        "        if any(ext in href for ext in ['.csv', '.xlsx', '.json', '.zip', '.txt', 'download', 'export', 'data']):\n",
        "            # Construct absolute URL if it's relative\n",
        "            if not href.startswith('http'):\n",
        "                # Simple join for relative paths, more complex logic needed for all cases\n",
        "                absolute_href = requests.compat.urljoin(SIATA_OPERACIONAL_URL, href)\n",
        "            else:\n",
        "                absolute_href = href\n",
        "            download_links.append(absolute_href)\n",
        "\n",
        "    if download_links:\n",
        "        print(\"\\nPotential downloadable data links found:\")\n",
        "        for dl_link in download_links:\n",
        "            print(f\"- {dl_link}\")\n",
        "            # 6. For simplicity, attempt to download the first identified relevant file for now\n",
        "            # In a real scenario, we might need to filter more specifically or present options.\n",
        "            if siata_downloaded_data is None: # Only download the first one for initial check\n",
        "                print(f\"Attempting to download data from: {dl_link}\")\n",
        "                data_file_response = requests.get(dl_link, allow_redirects=True, timeout=20)\n",
        "                data_file_response.raise_for_status()\n",
        "                siata_downloaded_data = data_file_response.content # Store raw bytes\n",
        "                print(f\"Successfully downloaded data from {dl_link}. Size: {len(siata_downloaded_data)} bytes.\")\n",
        "                # Decide if we want to break after first download or list all and then pick.\n",
        "                # For this step, let's download the first and stop.\n",
        "                # If the content type is text/json, we might try to decode it.\n",
        "                if 'json' in data_file_response.headers.get('Content-Type', ''):\n",
        "                    try:\n",
        "                        siata_downloaded_data = data_file_response.json() # Try parsing as JSON\n",
        "                        print(\"Downloaded data appears to be JSON.\")\n",
        "                    except json.JSONDecodeError:\n",
        "                        print(\"Downloaded data is not valid JSON, storing as raw content.\")\n",
        "                elif 'text' in data_file_response.headers.get('Content-Type', '') or 'csv' in data_file_response.headers.get('Content-Type', ''):\n",
        "                    try:\n",
        "                        siata_downloaded_data = data_file_response.text # Store as text for potential CSV/text parsing\n",
        "                        print(\"Downloaded data appears to be text/CSV.\")\n",
        "                    except Exception as parse_e:\n",
        "                        print(f\"Error parsing downloaded text: {parse_e}\")\n",
        "                break # Only download the first for now\n",
        "    else:\n",
        "        # 7. If no direct downloadable file link is found\n",
        "        print(\"\\nNo direct downloadable data links (.csv, .xlsx, .json, etc.) found on the page.\")\n",
        "        print(\"Manual inspection of the page or network requests might be necessary to locate the data source.\")\n",
        "\n",
        "except requests.exceptions.Timeout:\n",
        "    print(f\"Error: Request to {SIATA_OPERACIONAL_URL} timed out after 20 seconds.\")\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Error fetching data from SIATA operational URL: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred during processing: {e}\")\n",
        "\n",
        "if siata_downloaded_data is not None:\n",
        "    print(\"SIATA data successfully downloaded and stored in 'siata_downloaded_data'.\")\n",
        "else:\n",
        "    print(\"Failed to download SIATA data or no direct download links found.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c928c883",
        "outputId": "75b1f242-5d4d-42bd-a823-2b17ff0c5900"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Analyzing SIATA operational page for links...\n",
            "\n",
            "--- Link Analysis Summary ---\n",
            "No obvious data files found.\n",
            "\n",
            "Found 32 potential Subdirectories:\n",
            "- http://www.siata.gov.co/\n",
            "- https://www.siata.gov.co/\n",
            "- https://www.siata.gov.co/operacional/#\n",
            "- https://www.siata.gov.co/operacional/?C=M;O=A\n",
            "- https://www.siata.gov.co/operacional/?C=N;O=A\n",
            "- https://www.siata.gov.co/operacional/?C=S;O=A\n",
            "- https://www.siata.gov.co/operacional/CicloAnual/\n",
            "- https://www.siata.gov.co/operacional/Meteorologia/\n",
            "- https://www.siata.gov.co/operacional/WRF/\n",
            "- https://www.siata.gov.co/operacional/enso/\n",
            "- https://www.siata.gov.co/operacional/mapas/\n",
            "- https://www.siata.gov.co/operacional/monitoreo/\n",
            "- https://www.siata.gov.co/operacional/prcSirena/\n",
            "- https://www.siata.gov.co/operacional/radar/\n",
            "- https://www.siata.gov.co/operacional/radiometro/\n",
            "- https://www.siata.gov.co/operacional/seriesdetiempo/\n",
            "- https://www.siata.gov.co/operacional/seriesdetiempo_prueba/\n",
            "\n",
            "Link categorization complete.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "# Assuming siata_operacional_soup is available from the previous step\n",
        "# And SIATA_OPERACIONAL_URL is also available\n",
        "\n",
        "if siata_operacional_soup is None:\n",
        "    print(\"Error: siata_operacional_soup is not available. Please ensure the previous step ran successfully.\")\n",
        "else:\n",
        "    print(\"Analyzing SIATA operational page for links...\")\n",
        "\n",
        "    data_files = []\n",
        "    subdirectories = []\n",
        "    other_links = []\n",
        "\n",
        "    # Common data file extensions and keywords\n",
        "    data_extensions = ['.csv', '.xlsx', '.json', '.zip', '.txt', '.xml', '.kmz', '.tgz', '.gz']\n",
        "    data_keywords = ['data', 'export', 'archivo'] # in file name part\n",
        "\n",
        "    for link in siata_operacional_soup.find_all('a', href=True):\n",
        "        href = link['href']\n",
        "        absolute_href = requests.compat.urljoin(SIATA_OPERACIONAL_URL, href)\n",
        "\n",
        "        # Extract the path part of the URL to check for extensions/keywords\n",
        "        parsed_url = urlparse(absolute_href)\n",
        "        path = parsed_url.path.lower()\n",
        "\n",
        "        # 3. Categorize links\n",
        "        if path.endswith('/'):\n",
        "            subdirectories.append(absolute_href)\n",
        "        elif any(path.endswith(ext) for ext in data_extensions) or any(keyword in path for keyword in data_keywords):\n",
        "            data_files.append(absolute_href)\n",
        "        else:\n",
        "            other_links.append(absolute_href)\n",
        "\n",
        "    print(\"\\n--- Link Analysis Summary ---\")\n",
        "\n",
        "    if data_files:\n",
        "        print(f\"Found {len(data_files)} potential Data Files:\")\n",
        "        for df_link in sorted(list(set(data_files))): # Use set to avoid duplicates and sort for readability\n",
        "            print(f\"- {df_link}\")\n",
        "    else:\n",
        "        print(\"No obvious data files found.\")\n",
        "\n",
        "    if subdirectories:\n",
        "        print(f\"\\nFound {len(subdirectories)} potential Subdirectories:\")\n",
        "        for sub_link in sorted(list(set(subdirectories))): # Use set to avoid duplicates and sort for readability\n",
        "            print(f\"- {sub_link}\")\n",
        "    else:\n",
        "        print(\"No obvious subdirectories found.\")\n",
        "\n",
        "    # Optionally, print other links for full transparency\n",
        "    # if other_links:\n",
        "    #     print(f\"\\nFound {len(other_links)} other links (not categorized as data files or subdirectories):\")\n",
        "    #     for other_link_item in sorted(list(set(other_links))):\n",
        "    #         print(f\"- {other_link_item}\")\n",
        "\n",
        "    # Store the categorized links for potential future steps if needed\n",
        "    siata_found_data_files = list(set(data_files))\n",
        "    siata_found_subdirectories = list(set(subdirectories))\n",
        "\n",
        "    print(\"\\nLink categorization complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9b1b0015",
        "outputId": "734a214f-8863-4f3c-b06b-950ddba07bd1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attempting to fetch data from SIATA Meteorological URL: https://www.siata.gov.co/operacional/Meteorologia/\n",
            "Successfully fetched SIATA Meteorological page from https://www.siata.gov.co/operacional/Meteorologia/.\n",
            "Title of the page: Listing folder\n",
            "HTML content parsed successfully.\n",
            "\n",
            "--- Link Analysis Summary for Meteorologia Directory ---\n",
            "No obvious data files found in this directory.\n",
            "\n",
            "Found 12 potential Subdirectories:\n",
            "- http://www.siata.gov.co/\n",
            "- https://www.siata.gov.co/\n",
            "- https://www.siata.gov.co/operacional/\n",
            "- https://www.siata.gov.co/operacional/Meteorologia/#\n",
            "- https://www.siata.gov.co/operacional/Meteorologia/?C=M;O=A\n",
            "- https://www.siata.gov.co/operacional/Meteorologia/?C=N;O=A\n",
            "- https://www.siata.gov.co/operacional/Meteorologia/?C=S;O=A\n",
            "- https://www.siata.gov.co/operacional/Meteorologia/AcumPrecipitacion/\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urlparse, urljoin\n",
        "\n",
        "# 1. Define the URL for SIATA's Meteorological subdirectory\n",
        "SIATA_METEOROLOGIA_URL = \"https://www.siata.gov.co/operacional/Meteorologia/\"\n",
        "\n",
        "print(f\"Attempting to fetch data from SIATA Meteorological URL: {SIATA_METEOROLOGIA_URL}\")\n",
        "\n",
        "siata_meteorologia_soup = None\n",
        "\n",
        "try:\n",
        "    # 2. Make an HTTP GET request to the URL, allowing redirects and including a timeout\n",
        "    response = requests.get(SIATA_METEOROLOGIA_URL, allow_redirects=True, timeout=20)\n",
        "    # 3. Implement robust error handling\n",
        "    response.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)\n",
        "\n",
        "    print(f\"Successfully fetched SIATA Meteorological page from {SIATA_METEOROLOGIA_URL}.\")\n",
        "\n",
        "    # 4. Parse the HTML content of the response\n",
        "    siata_meteorologia_soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    print(f\"Title of the page: {siata_meteorologia_soup.title.string if siata_meteorologia_soup.title else 'No title found'}\")\n",
        "    print(\"HTML content parsed successfully.\")\n",
        "\n",
        "    # 5. Extract all links found on the page.\n",
        "    data_files = []\n",
        "    subdirectories = []\n",
        "\n",
        "    # Common data file extensions and keywords\n",
        "    data_extensions = ['.csv', '.xlsx', '.json', '.zip', '.txt', '.xml', '.kmz', '.tgz', '.gz']\n",
        "    data_keywords = ['data', 'export', 'archivo'] # in file name part\n",
        "\n",
        "    for link in siata_meteorologia_soup.find_all('a', href=True):\n",
        "        href = link['href']\n",
        "        absolute_href = urljoin(SIATA_METEOROLOGIA_URL, href)\n",
        "\n",
        "        # Extract the path part of the URL to check for extensions/keywords\n",
        "        parsed_url = urlparse(absolute_href)\n",
        "        path = parsed_url.path.lower()\n",
        "\n",
        "        # 6. Categorize the extracted links\n",
        "        if path.endswith('/') and absolute_href != SIATA_METEOROLOGIA_URL and absolute_href != SIATA_METEOROLOGIA_URL + '../': # Exclude current dir and parent dir\n",
        "            subdirectories.append(absolute_href)\n",
        "        elif any(path.endswith(ext) for ext in data_extensions) or any(keyword in path for keyword in data_keywords):\n",
        "            data_files.append(absolute_href)\n",
        "\n",
        "    # 7. Print a summary of the findings\n",
        "    print(\"\\n--- Link Analysis Summary for Meteorologia Directory ---\")\n",
        "\n",
        "    if data_files:\n",
        "        print(f\"Found {len(data_files)} potential Data Files:\")\n",
        "        for df_link in sorted(list(set(data_files))): # Use set to avoid duplicates and sort for readability\n",
        "            print(f\"- {df_link}\")\n",
        "    else:\n",
        "        print(\"No obvious data files found in this directory.\")\n",
        "\n",
        "    if subdirectories:\n",
        "        print(f\"\\nFound {len(subdirectories)} potential Subdirectories:\")\n",
        "        for sub_link in sorted(list(set(subdirectories))): # Use set to avoid duplicates and sort for readability\n",
        "            print(f\"- {sub_link}\")\n",
        "    else:\n",
        "        print(\"No obvious subdirectories found in this directory.\")\n",
        "\n",
        "except requests.exceptions.Timeout:\n",
        "    print(f\"Error: Request to {SIATA_METEOROLOGIA_URL} timed out after 20 seconds.\")\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Error fetching data from SIATA Meteorological URL: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred during processing: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1c7b3a91",
        "outputId": "118a941a-56d7-4a31-f723-59628711b1a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attempting to fetch data from SIATA Accumulative Precipitation URL: https://www.siata.gov.co/operacional/Meteorologia/AcumPrecipitacion/\n",
            "Successfully fetched SIATA Accumulative Precipitation page from https://www.siata.gov.co/operacional/Meteorologia/AcumPrecipitacion/.\n",
            "Title of the page: Listing folder\n",
            "HTML content parsed successfully.\n",
            "\n",
            "--- Link Analysis Summary for AcumPrecipitacion Directory ---\n",
            "Found 22 potential Data Files:\n",
            "- https://www.siata.gov.co/operacional/Meteorologia/AcumPrecipitacion/DatosPacum_Abril2025.txt\n",
            "- https://www.siata.gov.co/operacional/Meteorologia/AcumPrecipitacion/DatosPacum_Agosto2025.txt\n",
            "- https://www.siata.gov.co/operacional/Meteorologia/AcumPrecipitacion/DatosPacum_Diciembre2025.txt\n",
            "- https://www.siata.gov.co/operacional/Meteorologia/AcumPrecipitacion/DatosPacum_Febrero2025.txt\n",
            "- https://www.siata.gov.co/operacional/Meteorologia/AcumPrecipitacion/DatosPacum_Julio2025.txt\n",
            "- https://www.siata.gov.co/operacional/Meteorologia/AcumPrecipitacion/DatosPacum_Junio2025.txt\n",
            "- https://www.siata.gov.co/operacional/Meteorologia/AcumPrecipitacion/DatosPacum_Marzo2025.txt\n",
            "- https://www.siata.gov.co/operacional/Meteorologia/AcumPrecipitacion/DatosPacum_Mayo2025.txt\n",
            "- https://www.siata.gov.co/operacional/Meteorologia/AcumPrecipitacion/DatosPacum_Noviembre2025.txt\n",
            "- https://www.siata.gov.co/operacional/Meteorologia/AcumPrecipitacion/DatosPacum_Octubre2025.txt\n",
            "- https://www.siata.gov.co/operacional/Meteorologia/AcumPrecipitacion/DatosPacum_Septiembre2025.txt\n",
            "\n",
            "Found 12 potential Subdirectories:\n",
            "- http://www.siata.gov.co/\n",
            "- https://www.siata.gov.co/\n",
            "- https://www.siata.gov.co/operacional/Meteorologia/\n",
            "- https://www.siata.gov.co/operacional/Meteorologia/AcumPrecipitacion/#\n",
            "- https://www.siata.gov.co/operacional/Meteorologia/AcumPrecipitacion/?C=M;O=A\n",
            "- https://www.siata.gov.co/operacional/Meteorologia/AcumPrecipitacion/?C=N;O=A\n",
            "- https://www.siata.gov.co/operacional/Meteorologia/AcumPrecipitacion/?C=S;O=A\n",
            "- https://www.siata.gov.co/operacional/Meteorologia/AcumPrecipitacion/ReporteMunicipios/\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urlparse, urljoin\n",
        "\n",
        "# 1. Define the URL for SIATA's Accumulative Precipitation subdirectory\n",
        "SIATA_ACUMPRECIPITACION_URL = \"https://www.siata.gov.co/operacional/Meteorologia/AcumPrecipitacion/\"\n",
        "\n",
        "print(f\"Attempting to fetch data from SIATA Accumulative Precipitation URL: {SIATA_ACUMPRECIPITACION_URL}\")\n",
        "\n",
        "siata_acumprecipitacion_soup = None\n",
        "\n",
        "try:\n",
        "    # 2. Make an HTTP GET request to the URL, allowing redirects and including a timeout\n",
        "    response = requests.get(SIATA_ACUMPRECIPITACION_URL, allow_redirects=True, timeout=20)\n",
        "    # 3. Implement robust error handling\n",
        "    response.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)\n",
        "\n",
        "    print(f\"Successfully fetched SIATA Accumulative Precipitation page from {SIATA_ACUMPRECIPITACION_URL}.\")\n",
        "\n",
        "    # 4. Parse the HTML content of the response\n",
        "    siata_acumprecipitacion_soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    print(f\"Title of the page: {siata_acumprecipitacion_soup.title.string if siata_acumprecipitacion_soup.title else 'No title found'}\")\n",
        "    print(\"HTML content parsed successfully.\")\n",
        "\n",
        "    # 5. Extract all links found on the page.\n",
        "    data_files = []\n",
        "    subdirectories = []\n",
        "\n",
        "    # Common data file extensions and keywords\n",
        "    data_extensions = ['.csv', '.xlsx', '.json', '.zip', '.txt', '.xml', '.kmz', '.tgz', '.gz']\n",
        "    data_keywords = ['data', 'export', 'archivo'] # in file name part\n",
        "\n",
        "    for link in siata_acumprecipitacion_soup.find_all('a', href=True):\n",
        "        href = link['href']\n",
        "        absolute_href = urljoin(SIATA_ACUMPRECIPITACION_URL, href)\n",
        "\n",
        "        # Extract the path part of the URL to check for extensions/keywords\n",
        "        parsed_url = urlparse(absolute_href)\n",
        "        path = parsed_url.path.lower()\n",
        "\n",
        "        # 6. Categorize the extracted links\n",
        "        if path.endswith('/') and absolute_href != SIATA_ACUMPRECIPITACION_URL and absolute_href != SIATA_ACUMPRECIPITACION_URL + '../': # Exclude current dir and parent dir\n",
        "            subdirectories.append(absolute_href)\n",
        "        elif any(path.endswith(ext) for ext in data_extensions) or any(keyword in path for keyword in data_keywords):\n",
        "            data_files.append(absolute_href)\n",
        "\n",
        "    # 7. Print a summary of the findings\n",
        "    print(\"\\n--- Link Analysis Summary for AcumPrecipitacion Directory ---\")\n",
        "\n",
        "    if data_files:\n",
        "        print(f\"Found {len(data_files)} potential Data Files:\")\n",
        "        for df_link in sorted(list(set(data_files))): # Use set to avoid duplicates and sort for readability\n",
        "            print(f\"- {df_link}\")\n",
        "    else:\n",
        "        print(\"No obvious data files found in this directory.\")\n",
        "\n",
        "    if subdirectories:\n",
        "        print(f\"\\nFound {len(subdirectories)} potential Subdirectories:\")\n",
        "        for sub_link in sorted(list(set(subdirectories))): # Use set to avoid duplicates and sort for readability\n",
        "            print(f\"- {sub_link}\")\n",
        "    else:\n",
        "        print(\"No obvious subdirectories found in this directory.\")\n",
        "\n",
        "except requests.exceptions.Timeout:\n",
        "    print(f\"Error: Request to {SIATA_ACUMPRECIPITACION_URL} timed out after 20 seconds.\")\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Error fetching data from SIATA Accumulative Precipitation URL: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred during processing: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9b99732e",
        "outputId": "2696aff1-2a24-48d1-896f-e8d6f9c6261d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attempting to download data file from: https://www.siata.gov.co/operacional/Meteorologia/AcumPrecipitacion/DatosPacum_Noviembre2025.txt\n",
            "Successfully downloaded data from https://www.siata.gov.co/operacional/Meteorologia/AcumPrecipitacion/DatosPacum_Noviembre2025.txt.\n",
            "\n",
            "--- Sample of raw file content (first 500 characters) ---\n",
            "Fecha actualizacion: 2025/12/01 00:01\n",
            "Estacion,Nombre,Municipio,Barrio,Climatologia mes,Acumulado Mes (mm),Porcentaje Mes\n",
            "66, I.E San Andres (Sede El Socorro), Girardota, NA, 120.730, 284.734, 235.844\n",
            "420, Pueblo Viejo - Pluviometro, La Estrella, NULL, 172.720, 382.016, 221.176\n",
            "278, Vereda Potrerito - Pluviometro, Barbosa, NA, 229.210, 429.260, 187.278\n",
            "127, I.E. Manuel Jose Sierra - Sede la Holanda, Girardota, NA, 146.710, 267.716, 182.480\n",
            "62, Gimnasio Cantabria, La Estrella, NA, 208.720, 370.07\n",
            "-----------------------------------------------------------\n",
            "\n",
            "Successfully parsed data into a DataFrame (assuming tab-separated).\n",
            "First 5 rows of SIATA precipitation DataFrame:\n",
            "               Fecha actualizacion: 2025/12/01 00:01\n",
            "0  Estacion,Nombre,Municipio,Barrio,Climatologia ...\n",
            "1  66, I.E San Andres (Sede El Socorro), Girardot...\n",
            "2  420, Pueblo Viejo - Pluviometro, La Estrella, ...\n",
            "3  278, Vereda Potrerito - Pluviometro, Barbosa, ...\n",
            "4  127, I.E. Manuel Jose Sierra - Sede la Holanda...\n",
            "\n",
            "DataFrame Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 126 entries, 0 to 125\n",
            "Data columns (total 1 columns):\n",
            " #   Column                                 Non-Null Count  Dtype \n",
            "---  ------                                 --------------  ----- \n",
            " 0   Fecha actualizacion: 2025/12/01 00:01  126 non-null    object\n",
            "dtypes: object(1)\n",
            "memory usage: 1.1+ KB\n",
            "SIATA precipitation data successfully loaded into 'siata_precipitacion_data' DataFrame.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import io\n",
        "\n",
        "# URL of a specific data file identified in the previous step\n",
        "# Choosing DatosPacum_Noviembre2025.txt as an example\n",
        "SIATA_DATA_FILE_URL = \"https://www.siata.gov.co/operacional/Meteorologia/AcumPrecipitacion/DatosPacum_Noviembre2025.txt\"\n",
        "\n",
        "print(f\"Attempting to download data file from: {SIATA_DATA_FILE_URL}\")\n",
        "\n",
        "siata_precipitacion_data = None\n",
        "\n",
        "try:\n",
        "    # Make an HTTP GET request to download the file\n",
        "    response = requests.get(SIATA_DATA_FILE_URL, allow_redirects=True, timeout=30)\n",
        "    response.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)\n",
        "\n",
        "    print(f\"Successfully downloaded data from {SIATA_DATA_FILE_URL}.\")\n",
        "\n",
        "    # The content is text, likely tab-separated based on typical SIATA files or simple text\n",
        "    # Let's try to read it into a pandas DataFrame, first as a raw string to inspect.\n",
        "    raw_content = response.text\n",
        "    print(\"\\n--- Sample of raw file content (first 500 characters) ---\")\n",
        "    print(raw_content[:500])\n",
        "    print(\"-----------------------------------------------------------\")\n",
        "\n",
        "    # Attempt to parse as CSV/TXT, assuming tab or space separated for now\n",
        "    # Given the output format, it's highly likely to be tab-separated or fixed-width.\n",
        "    # Let's try reading with pandas, inferring delimiter or using common ones.\n",
        "    try:\n",
        "        # Using io.StringIO to treat the string content as a file\n",
        "        siata_precipitacion_data = pd.read_csv(io.StringIO(raw_content), sep='\\t', encoding='utf-8')\n",
        "        print(\"\\nSuccessfully parsed data into a DataFrame (assuming tab-separated).\")\n",
        "        print(\"First 5 rows of SIATA precipitation DataFrame:\")\n",
        "        print(siata_precipitacion_data.head())\n",
        "        print(\"\\nDataFrame Info:\")\n",
        "        siata_precipitacion_data.info()\n",
        "    except Exception as parse_error:\n",
        "        print(f\"Error parsing data into DataFrame: {parse_error}\")\n",
        "        print(\"Attempting to read with space as delimiter.\")\n",
        "        try:\n",
        "            siata_precipitacion_data = pd.read_csv(io.StringIO(raw_content), sep=' ', skipinitialspace=True, encoding='utf-8')\n",
        "            print(\"\\nSuccessfully parsed data into a DataFrame (assuming space-separated).\")\n",
        "            print(\"First 5 rows of SIATA precipitation DataFrame:\")\n",
        "            print(siata_precipitacion_data.head())\n",
        "            print(\"\\nDataFrame Info:\")\n",
        "            siata_precipitacion_data.info()\n",
        "        except Exception as second_parse_error:\n",
        "            print(f\"Error parsing data with space delimiter: {second_parse_error}\")\n",
        "            print(\"Could not parse data into DataFrame. Raw content stored as string.\")\n",
        "            siata_precipitacion_data = raw_content # Fallback to raw string if parsing fails\n",
        "\n",
        "except requests.exceptions.Timeout:\n",
        "    print(f\"Error: Request to {SIATA_DATA_FILE_URL} timed out after 30 seconds.\")\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Error fetching data file from SIATA: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred during processing: {e}\")\n",
        "\n",
        "if siata_precipitacion_data is not None and isinstance(siata_precipitacion_data, pd.DataFrame):\n",
        "    print(\"SIATA precipitation data successfully loaded into 'siata_precipitacion_data' DataFrame.\")\n",
        "elif siata_precipitacion_data is not None:\n",
        "    print(\"SIATA precipitation data downloaded but not parsed into DataFrame. Stored as raw content.\")\n",
        "else:\n",
        "    print(\"Failed to download or process SIATA precipitation data.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5140a797",
        "outputId": "9000836f-cf81-4321-bb92-761c2d472cca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attempting to re-parse SIATA precipitation data with correct delimiter and skipping header.\n",
            "\n",
            "Successfully parsed data into a DataFrame with correct delimiter and skipped header.\n",
            "First 5 rows of cleaned SIATA precipitation DataFrame:\n",
            "   Estacion                                      Nombre     Municipio Barrio  \\\n",
            "0        66            I.E San Andres (Sede El Socorro)     Girardota     NA   \n",
            "1       420                  Pueblo Viejo - Pluviometro   La Estrella   NULL   \n",
            "2       278              Vereda Potrerito - Pluviometro       Barbosa     NA   \n",
            "3       127   I.E. Manuel Jose Sierra - Sede la Holanda     Girardota     NA   \n",
            "4        62                          Gimnasio Cantabria   La Estrella     NA   \n",
            "\n",
            "   Climatologia mes  Acumulado Mes (mm)  Porcentaje Mes  \n",
            "0            120.73             284.734         235.844  \n",
            "1            172.72             382.016         221.176  \n",
            "2            229.21             429.260         187.278  \n",
            "3            146.71             267.716         182.480  \n",
            "4            208.72             370.078         177.308  \n",
            "\n",
            "DataFrame Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 125 entries, 0 to 124\n",
            "Data columns (total 7 columns):\n",
            " #   Column              Non-Null Count  Dtype  \n",
            "---  ------              --------------  -----  \n",
            " 0   Estacion            125 non-null    int64  \n",
            " 1   Nombre              125 non-null    object \n",
            " 2   Municipio           125 non-null    object \n",
            " 3   Barrio              125 non-null    object \n",
            " 4   Climatologia mes    125 non-null    float64\n",
            " 5   Acumulado Mes (mm)  125 non-null    float64\n",
            " 6   Porcentaje Mes      125 non-null    float64\n",
            "dtypes: float64(3), int64(1), object(3)\n",
            "memory usage: 7.0+ KB\n",
            "SIATA precipitation data successfully loaded into 'siata_precipitacion_data' DataFrame.\n"
          ]
        }
      ],
      "source": [
        "import io\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming raw_content is available from the previous step\n",
        "# It contains the entire text content of the downloaded file.\n",
        "\n",
        "if raw_content:\n",
        "    print(\"Attempting to re-parse SIATA precipitation data with correct delimiter and skipping header.\")\n",
        "    try:\n",
        "        # Use io.StringIO to treat the string content as a file\n",
        "        # Skip the first row (metadata) using skiprows=1\n",
        "        # Use comma as the delimiter\n",
        "        siata_precipitacion_data_cleaned = pd.read_csv(\n",
        "            io.StringIO(raw_content),\n",
        "            sep=',',\n",
        "            skiprows=1, # Skip the \"Fecha actualizacion\" line\n",
        "            encoding='utf-8',\n",
        "            # We might have issues with extra spaces or inconsistent delimiters, let's refine this if needed\n",
        "            # For now, assuming standard CSV format after skipping the first line.\n",
        "            on_bad_lines='skip' # Skip lines that have too many fields\n",
        "        )\n",
        "        print(\"\\nSuccessfully parsed data into a DataFrame with correct delimiter and skipped header.\")\n",
        "        print(\"First 5 rows of cleaned SIATA precipitation DataFrame:\")\n",
        "        print(siata_precipitacion_data_cleaned.head())\n",
        "        print(\"\\nDataFrame Info:\")\n",
        "        siata_precipitacion_data_cleaned.info()\n",
        "\n",
        "        siata_precipitacion_data = siata_precipitacion_data_cleaned # Update the main variable\n",
        "\n",
        "    except Exception as parse_error:\n",
        "        print(f\"Error re-parsing data into DataFrame: {parse_error}\")\n",
        "        print(\"Could not parse data into DataFrame after cleaning attempt. Raw content stored as string.\")\n",
        "        # Fallback to raw string if parsing fails even after cleaning attempts\n",
        "        siata_precipitacion_data = raw_content\n",
        "else:\n",
        "    print(\"No raw content available for re-parsing.\")\n",
        "\n",
        "if siata_precipitacion_data is not None and isinstance(siata_precipitacion_data, pd.DataFrame):\n",
        "    print(\"SIATA precipitation data successfully loaded into 'siata_precipitacion_data' DataFrame.\")\n",
        "elif siata_precipitacion_data is not None:\n",
        "    print(\"SIATA precipitation data downloaded but not parsed into DataFrame. Stored as raw content.\")\n",
        "else:\n",
        "    print(\"Failed to download or process SIATA precipitation data.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
