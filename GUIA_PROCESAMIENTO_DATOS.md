# ğŸ“Š GuÃ­a de Procesamiento de Datos ClimÃ¡ticos

## ğŸ¯ Resumen Ejecutivo

Se ha implementado un **pipeline ETL modular** para procesar datos climÃ¡ticos sin alterar la estructura existente del proyecto. Los datos fluyen asÃ­:

```
ğŸ“ Data (JSON, CSV, TXT, ImÃ¡genes)
    â†“
ğŸ“¥ Data Loaders (Parsear mÃºltiples formatos)
    â†“
âœ… Validators (Detectar anomalÃ­as)
    â†“
ğŸ”„ Pipelines (Orquestar proceso completo)
    â†“
ğŸ“Š DataFrame Limpio (Listo para anÃ¡lisis)
```

---

## ğŸ“‚ Estructura Nueva

```
src/
â”œâ”€â”€ data_loaders/          â† ğŸ†• NUEVO
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ json_loader.py      # Parsea JSON de APIs climÃ¡ticas
â”‚   â”œâ”€â”€ file_loader.py      # CSV, TXT, Excel
â”‚   â””â”€â”€ unified_loader.py   # Cargador unificado (punto de entrada)
â”‚
â”œâ”€â”€ validators/            â† ğŸ†• NUEVO
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ data_validator.py   # ValidaciÃ³n y limpieza de datos
â”‚
â”œâ”€â”€ pipelines/             â† ğŸ†• NUEVO
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ climate_pipeline.py # Orquesta todo el proceso
â”‚
â”œâ”€â”€ processors/            â† Existente (sin cambios)
â”œâ”€â”€ visualizers/           â† Existente (sin cambios)
â””â”€â”€ data_sources/          â† Existente (sin cambios)
```

---

## ğŸš€ Uso RÃ¡pido

### **OpciÃ³n 1: Lo mÃ¡s simple (1 lÃ­nea)**
```python
from src.data_loaders import UnifiedDataLoader

# Carga TODO
df = UnifiedDataLoader("data").load_all()
print(df.shape)  # (N registros, M columnas)
```

### **OpciÃ³n 2: Recomendado (pipeline completo)**
```python
from src.pipelines import ClimateDataPipeline

pipeline = ClimateDataPipeline("data")
df_clean = pipeline.execute(
    validate=True,          # Eliminar outliers
    fill_nulls=True,        # Rellenar nulos
    remove_outliers=True,   # Validar rangos
    resample_freq=None      # Sin resampleo (o '1H' para horario)
)
```

### **OpciÃ³n 3: Por ubicaciÃ³n**
```python
pipeline = ClimateDataPipeline("data")

# Listar ubicaciones disponibles
locations = UnifiedDataLoader.get_available_locations("data")
# Resultado: ['Bogota', 'Cali', 'Cartagena', 'MedellÃ­n', ...]

# Procesar una ubicaciÃ³n
df_bogota = pipeline.execute_by_location("Bogota")
```

---

## ğŸ“Š MÃ³dulos Detallados

### **1. Data Loaders** (`src/data_loaders/`)

#### `JSONDataLoader` - Parsea APIs climÃ¡ticas
```python
from src.data_loaders import JSONDataLoader

# Cargar un JSON especÃ­fico
data = JSONDataLoader.load_json("data/consulta_completa_Bogota.json")

# Extraer datos de Meteoblue
df_meteoblue = JSONDataLoader.extract_meteoblue(data, location="Bogota")

# Extraer datos de OpenMeteo
df_openmeteo = JSONDataLoader.extract_openmeteo(data, location="Bogota")

# Cargar directorio completo
df = JSONDataLoader.load_from_directory("data", pattern="consulta_completa_*.json")
```

**Columnas generadas:**
- `timestamp` - Fecha/hora
- `temperature_C` - Temperatura (Â°C)
- `windspeed_ms` - Velocidad viento (m/s)
- `winddirection_deg` - DirecciÃ³n viento (Â°)
- `precipitation_mm` - PrecipitaciÃ³n (mm)
- `humidity_percent` - Humedad (%)
- `pressure_hPa` - PresiÃ³n (hPa)
- `source` - Fuente (meteoblue, openmeteo, etc)

#### `FileLoader` - CSV, TXT, Excel
```python
from src.data_loaders import FileLoader

# Cargar archivo individual
df = FileLoader.load_file("data/datos_clima.csv")

# Cargar directorio completo
archivos = FileLoader.load_directory("data", pattern="*.csv")

# Estandarizar nombres de columnas
df_std = FileLoader.standardize_columns(df)
```

#### `UnifiedDataLoader` - Punto Ãºnico de entrada
```python
from src.data_loaders import UnifiedDataLoader

loader = UnifiedDataLoader("data")

# Cargar todo (JSON + CSV + TXT)
df = loader.load_all(
    standardize=True,   # Nombres estÃ¡ndar
    remove_nulls=True,  # Filas completamente vacÃ­as
    resample_freq=None  # '1H' = horario, 'D' = diario
)

# Cargar por ubicaciÃ³n
df_bogota = loader.load_location("Bogota")

# Cargar por fuente
df_meteoblue = loader.load_source("meteoblue")

# Listar ubicaciones/fuentes disponibles
locations = UnifiedDataLoader.get_available_locations("data")
sources = UnifiedDataLoader.get_available_sources("data")
```

---

### **2. Validators** (`src/validators/`)

#### `DataValidator` - Limpieza y validaciÃ³n
```python
from src.validators import DataValidator

# Validar rango de temperatura (-50 a 60Â°C)
df_valid, report = DataValidator.validate_range(
    df, 'temperature_C', -50, 60
)

# Validar TODO contra rangos conocidos
df_clean, reports = DataValidator.validate_all(df)

# Detectar datos faltantes
missing = DataValidator.check_missing_data(df)
# Resultado: {'temperature_C': 2.5, 'pressure_hPa': 1.2, ...}

# Rellenar nulos
df_filled = DataValidator.fill_missing(df, method='linear')
# Methods: 'forward', 'linear', 'mean', 'drop'

# Detectar duplicados
n_duplicates = DataValidator.detect_duplicates(df)
```

**Rangos validados automÃ¡ticamente:**
```python
VALID_RANGES = {
    'temperature_C': (-50, 60),
    'windspeed_ms': (0, 50),
    'humidity_percent': (0, 100),
    'pressure_hPa': (900, 1100),
    'precipitation_mm': (0, 500),
    'cloudiness_percent': (0, 100),
}
```

---

### **3. Pipelines** (`src/pipelines/`)

#### `ClimateDataPipeline` - Orquesta todo
```python
from src.pipelines import ClimateDataPipeline

pipeline = ClimateDataPipeline("data")

# EXECUTE: Carga â†’ Valida â†’ Llena nulos â†’ Elimina duplicados
df = pipeline.execute(
    validate=True,
    fill_nulls=True,
    remove_outliers=True,
    resample_freq='1H'  # Resamplear a horario
)

# Por ubicaciÃ³n
df_cali = pipeline.execute_by_location("Cali", fill_nulls=True)

# Por fuente
df_meteoblue = pipeline.execute_by_source("meteoblue")

# Guardar resultado
output_path = pipeline.save_processed(df)
# Genera: data/processed/clima_procesado_YYYYMMDD_HHMMSS.csv
```

---

## ğŸ“ˆ Ejemplo Completo

```python
import pandas as pd
from src.pipelines import ClimateDataPipeline

# 1. Crear pipeline
pipeline = ClimateDataPipeline("data")

# 2. Procesar datos
df = pipeline.execute(
    validate=True,
    fill_nulls=True,
    remove_outliers=True
)

# 3. AnÃ¡lisis
print(f"Registros: {len(df)}")
print(f"Columnas: {df.columns.tolist()}")
print(f"PerÃ­odo: {df['timestamp'].min()} â†’ {df['timestamp'].max()}")

# 4. EstadÃ­sticas por variable
print(df.describe())

# 5. CorrelaciÃ³n
numeric = df.select_dtypes(include=['number']).columns
print(df[numeric].corr())

# 6. Agrupar por ubicaciÃ³n
for location in df['location'].unique():
    df_loc = df[df['location'] == location]
    print(f"\n{location}: {len(df_loc)} registros")
    print(f"  Temperatura: {df_loc['temperature_C'].min():.1f} - {df_loc['temperature_C'].max():.1f}Â°C")
    print(f"  Viento: {df_loc['windspeed_ms'].mean():.1f} m/s promedio")

# 7. Guardar
pipeline.save_processed(df)
```

---

## ğŸ” Casos de Uso

### **Caso 1: AnÃ¡lisis temporal**
```python
df = pipeline.execute(resample_freq='1H')  # Datos horarios

# Temperatura promedio por hora
temp_horaria = df.groupby(df['timestamp'].dt.hour)['temperature_C'].mean()

# Viento mÃ¡ximo diario
df['fecha'] = df['timestamp'].dt.date
viento_max = df.groupby('fecha')['windspeed_ms'].max()
```

### **Caso 2: ComparaciÃ³n entre ubicaciones**
```python
for location in UnifiedDataLoader.get_available_locations("data"):
    df_loc = pipeline.execute_by_location(location)
    
    print(f"{location}:")
    print(f"  Temp prom: {df_loc['temperature_C'].mean():.1f}Â°C")
    print(f"  Humedad prom: {df_loc['humidity_percent'].mean():.1f}%")
    print(f"  PrecipitaciÃ³n total: {df_loc['precipitation_mm'].sum():.1f}mm")
```

### **Caso 3: AnÃ¡lisis de calidad del aire (si disponible)**
```python
df = pipeline.execute()

# Filtrar por variables de aire
air_cols = [col for col in df.columns if 'aqi' in col.lower() or 'pm' in col.lower()]
if air_cols:
    print("Variables de aire disponibles:", air_cols)
    print(df[air_cols].describe())
```

---

## âš™ï¸ ConfiguraciÃ³n

### **Cambiar directorio de datos**
```python
pipeline = ClimateDataPipeline("data/clima")  # Otro directorio
```

### **MÃ©todos de rellenado de nulos**
```python
# Forward fill (Ãºltima observaciÃ³n)
df = DataValidator.fill_missing(df, method='forward')

# InterpolaciÃ³n lineal
df = DataValidator.fill_missing(df, method='linear')

# Media de columna
df = DataValidator.fill_missing(df, method='mean')

# Eliminar filas nulas
df = DataValidator.fill_missing(df, method='drop')
```

### **Crear rango personalizado**
```python
from src.validators import DataValidator

# Validar temperatura entre -10 y 40Â°C
df_filtered, report = DataValidator.validate_range(
    df, 'temperature_C', min_val=-10, max_val=40
)
print(report)
# {'column': 'temperature_C', 'outliers_count': 25, 'outliers_percent': 0.5, ...}
```

---

## ğŸ“ Logging

El sistema registra automÃ¡ticamente todos los pasos:

```python
import logging

# Habilitar logging detallado
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

pipeline = ClimateDataPipeline("data")
df = pipeline.execute()

# Salida:
# [INFO] ============================================================
# [INFO] Cargando archivos JSON...
# [INFO] âœ“ JSON: 1000 registros
# [INFO] Validando datos...
# [INFO] Eliminadas 5 filas con outliers
# ...
```

---

## ğŸ”— IntegraciÃ³n con Proyecto

Sin cambios en:
- âœ… `src/data_sources/` - APIs siguen igual
- âœ… `src/processors/` - Radar processor intacto
- âœ… `src/visualizers/` - Visualizadores intactos
- âœ… `main.py` - Script principal sin tocar

AÃ±ade:
- ğŸ†• `src/data_loaders/` - Nuevos loaders
- ğŸ†• `src/validators/` - Validadores
- ğŸ†• `src/pipelines/` - Pipelines ETL

---

## ğŸš¨ Errores Comunes

### **"MÃ³dulo no encontrado"**
```python
# AsegÃºrate de ejecutar desde raÃ­z del proyecto
import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent))
```

### **No se cargan JSON**
```python
# Verifica patrÃ³n de archivos
from pathlib import Path
json_files = list(Path("data").glob("*.json"))
print(f"JSONs encontrados: {len(json_files)}")

# Puede necesitar patrÃ³n especÃ­fico
json_files = list(Path("data").glob("consulta_completa_*.json"))
```

### **Muchos nulos despuÃ©s de limpiar**
```python
# No rellenar automÃ¡ticamente
df = pipeline.execute(fill_nulls=False)
print(df.isna().sum())  # Ver dÃ³nde estÃ¡n los nulos

# O cambiar mÃ©todo de relleno
df = DataValidator.fill_missing(df, method='mean')
```

---

## ğŸ“š Referencias

- Pandas: https://pandas.pydata.org/
- NumPy: https://numpy.org/
- Datos meteorolÃ³gicos: https://openmeteo.com/, https://www.meteoblue.com/

---

## âœ¨ PrÃ³ximos Pasos

1. **Machine Learning**: `src/ml_models/` para predicciones
2. **AnÃ¡lisis Avanzado**: `src/analysis/` para correlaciones, trends
3. **Dashboard DinÃ¡mico**: Integrar con Streamlit existente
4. **Almacenamiento**: Base de datos para histÃ³rico

Ejecuta ahora:
```bash
python ejemplo_procesamiento.py
```
